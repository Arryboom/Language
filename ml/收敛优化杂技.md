#模型不收敛的原因

1.数据量少的话，一般不会带来不收敛
不收敛一般是由于样本的信息量太大导致网络不足以fit住整个样本空间，样本少只可能带来过拟合的问题。
降低过拟合的方法：比如dropout，SGD，增大minibatch的数量，减少fc层的节点数量，momentum，finetune等。

2.尽量收集更多的数据

3.尽可能用小模型
数据量太少应考虑缩小模型复杂度。考虑减少层数或者减少kernel number



#tensorflow使用高阶api导致训练不收敛问题




>https://blog.csdn.net/chongtong/article/details/90369917?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-21&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-21


上图是tensorflow软件栈图，我之前学习和实现的网络模型（0.12a）使用的是 低级api, 现在的新版本（1.10）对低级api进行了封装，形成了高级api(estimator keras)，所以对原有模型进行了一次api替换

### 移植

整个代码的移植过程还是比较顺利的，移植完成以后，代码量比以前减少了很多，但移植完成，在运行时却发现了一些奇怪的现象

### 问题现象

我这里有两个自己的数据集，其中一个数据集（单个数据量130）在移植后的代码上运行正常，另一个数据集（单个数据集60000）在移植后的代码上运行却出现以下问题：

-   二分类问题的准确率在50%左右
-   训练过程中loss值会变化到一个固定值，然后就不再变化了


### 问题分析

因为不同数据集对应的结论不同，所以问题的排查就主要集中在对比两份代码的差异上了。训练正常的代码简称为T代码，训练异常的代码简称为F代码

-   网络排查
    
    > 怀疑模型代码有问题。将F代码尽量用T代码代替，最终替换后，只剩下tfrecord文件读取和解析、网络输入层不一样，然后再训练更新后的F代码，现象依然存在
    
-   数据排查
    
    > 通过网络排查基本排除网络问题，唯一不同在于数据，于是进行数据验证。将F代码训练过程中的输入数据记录到文件，然后对比F代码读到的数据和制作tfrecord的数据。排查数据编号、数据内容是否一致。最终发现数据一致。


新代码中使用的损失函数是`tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)`，搜索并替换为[tensorflow官方module库](https://github.com/tensorflow/models)中的损失函数`tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)`再进行训练，发现一切正常





#SSD-TensorFlow不收敛的问题
第一次发~  
主要是解决初学者使用SSD-TensorFlow不收敛的问题。  
[SSD-TensorFlow下载地址](https://github.com/balancap/SSD-Tensorflow)  
[加载模型对图片进行测试](https://blog.csdn.net/duanyajun987/article/details/82786953)

许多初学者在训练时会发现SSD-TensorFlow的loss非常大的问题，其实github里已有人找到这个问题并解决了。具体做法是将nets->ssd\_vgg\_300.py的637-647行，如下代码 # Add cross-entropy loss.  
with tf.name\_scope(‘cross\_entropy\_pos’):  
loss = tf.nn.sparse\_softmax\_cross\_entropy\_with\_logits(logits=logits,  
labels=gclasses)  
loss = tf.div(tf.reduce\_sum(loss \* fpmask), batch\_size, name=‘value’)  
tf.losses.add\_loss(loss)  
with tf.name\_scope(‘cross\_entropy\_neg’):  
loss = tf.nn.sparse\_softmax\_cross\_entropy\_with\_logits(logits=logits,  
labels=no\_classes)  
loss = tf.div(tf.reduce\_sum(loss \* fnmask), batch\_size, name=‘value’)  
tf.losses.add\_loss(loss)  
改为：  
fn\_neg = tf.cast(n\_neg, dtype)  
fn\_positives = tf.cast(n\_positives, dtype)  
with tf.name\_scope(‘cross\_entropy\_pos’):  
loss = tf.nn.sparse\_softmax\_cross\_entropy\_with\_logits(logits=logits,  
labels=gclasses)  
loss = tf.div(tf.reduce\_sum(loss \* fpmask), fn\_positives, name=‘value’)  
tf.losses.add\_loss(loss)  
with tf.name\_scope(‘cross\_entropy\_neg’):  
loss = tf.nn.sparse\_softmax\_cross\_entropy\_with\_logits(logits=logits,  
labels=no\_classes)  
loss = tf.div(tf.reduce\_sum(loss \* fnmask), fn\_neg, name=‘value’)  
tf.losses.add\_loss(loss)  
具体解释在github里，这里附上链接[模型收敛问题](https://github.com/balancap/SSD-Tensorflow/issues/279)



#深度学习训练时网络不收敛的原因分析总结

很多同学会发现，为什么我训练网络的时候loss一直居高不下或者准确度时高时低，震荡趋势，一会到11，一会又0.1，不收敛。 又不知如何解决，博主总结了自己训练经验和看到的一些方法。

首先你要保证训练的次数够多，不要以为一百两百次就会一直loss下降或者准确率一直提高，会有一点震荡的。只要总体收敛就行。若训练次数够多（一般上千次，上万次，或者几十个epoch）没收敛，则试试下面方法：

## **1\. 数据和标签**

数据分类标注是否准确？数据是否干净？数据库太小一般不会带来不收敛的问题，只要你一直在train总会收敛（rp问题跑飞了不算）。反而不收敛一般是由于样本的信息量太大导致网络不足以fit住整个样本空间。样本少只可能带来过拟合的问题

## 2\. 学习率设定不合理

在自己训练新网络时，可以从0.1开始尝试，如果loss不下降的意思，那就降低，除以10，用0.01尝试，一般来说0.01会收敛，不行的话就用0.001. 学习率设置过大，很容易震荡。不过刚刚开始不建议把学习率设置过小，尤其是在训练的开始阶段。在开始阶段我们不能把学习率设置的太低否则loss不会收敛。我的做法是逐渐尝试，从0.1,0.08,0.06,0.05 ......逐渐减小直到正常为止，

有的时候候学习率太低走不出低估，把冲量提高也是一种方法，适当提高mini-batch值，使其波动不大。,

##  **3.网络设定不合理**

如果做很复杂的分类任务，却只用了很浅的网络，可能会导致训练难以收敛，换网络换网络换网络，重要的事情说三遍，或者也可以尝试加深当前网络。

## 4.数据集label的设置

检查lable是否有错，有的时候图像类别的label设置成1，2，3正确设置应该为0,1,2。

## 5、改变图片大小

博主看到一篇文章，说改变图片大小可以解决收敛问题，具体博主没试过，只看到有这个方法，具体文章链接：[https://blog.csdn.net/Fighting\_Dreamer/article/details/71498256](https://blog.csdn.net/Fighting_Dreamer/article/details/71498256)

感兴趣的可以去看看。

## 6、数据归一化

神经网络中对数据进行归一化是不可忽略的步骤，网络能不能正常工作，还得看你有没有做归一化，一般来讲，归一化就是减去数据平均值除以标准差，通常是针对每个输入和输出特征进行归一化




#有可能的原因之一
不请自来，随便逛看到了这个问题没人回答。

导致不收敛情况的原因有很多种，其中一种是一些样本正向传播到最外层时输出为0了，导致log(0)的出现，随之而来的就是loss为nan，这也是一个小陷阱。

根据题主给的代码，我认为这个原因导致问题出现的可能性比较大。

解决办法也比较有意思，就是给log里加一个小余项，具体到题主的代码就是：

将cross\_entropy = -tf.reduce\_sum(y\_\*tf.log(y))  
改为：  
cross\_entropy = -tf.reduce\_sum(y\_\*tf.log(y+1e-10))

如果一切顺利，应该不会出现loss为nan的情况了，题主可以试一试~



#内存泄漏

计算机配置内存12G，显存4G，运行有10分钟左右就提醒说内存不够用然后退出运行，直觉是程序bug问题
终于找到靠谱解决方案：
http://cherishlc.iteye.com/blog/2324796
这个博客的第二个给了大致的讲解，在给出的附录中：
https://stackoverflow.com/documentation/tensorflow/3883/how-to-debug-a-memory-leak-in-tensorflow/13426/use-graph-finalize-to-catch-nodes-being-added-to-the-graph#t=201707221208374841351
讲解更详细，并给出了解决的方法
即：sess.graph.finalize() 使得整个graph变为只读的，不能再向图中添加任何节点
我也明白了我的错误原因，在for循环中用了tf.convert_to_tensor方法，相当于不停地向图中添加节点，谢谢给出了解决方案的大牛们
另一个附录：https://github.com/tensorflow/tensorflow/issues/4151
通过一个示例给出了可行的替换方法，非常具有可行性




session和graph没有释放内存。使用了with关键字可以在session异常退出时也释放内存，否则要用session.close()关闭session。代码如下：

session一定要释放：

```
with tf.Seesion() as session:
    # 在session内部加载保存好的graph
    saver = tf.train.import_meta_graph('./crack_captcha.model-9900.meta')
    saver.restore(session, "./crack_captcha.model-9900.meta")
    # codes
```

在session中加载graph（训练好的模型），导致每次关闭程序再运行，graph出现重复加载的现象（尤其是重复加载模型的时候容易导致内存溢出）：

```
# 用with新建一个graph，这样在运行完以及异常退出时就会释放内存
graph = tf.Graph()
with graph.as_default():
    saver = tf.train.import_meta_graph('./crack_captcha.model-9900.meta')
    with tf.Session(graph=graph) as session:
        saver.restore(session, "./CNN_cracks")
```








**1、过拟合问题**

 **欠拟合：**根本原因是特征维度过少，模型过于简单，导致拟合的函数无法满足训练集，误差较大；   
　　　　　　解决方法：增加特征维度，增加训练数据；   
　　**过拟合：**根本原因是**特征维度过多，模型假设过于复杂，参数过多，训练数据过少，噪声过多**，导致拟合的函数完美的预测训练集，但对新数据的测试集预测结果差。 过度的拟合了训练数据，而没有考虑到泛化能力。  
　　　　　　解决方法：（1）减少特征维度；（2）正则化，降低参数值。

 **减少过拟合总结：**过拟合主要是有两个原因造成的：数据太少+模型太复杂   
　　（1）获取更多数据 ：从数据源头获取更多数据；数据增强（Data Augmentation）   
　　（2）使用合适的模型：减少网络的层数、神经元个数等均可以限制网络的拟合能力；   
　　（3）_dropout_ ；  
　　（4）正则化，在训练的时候限制权值变大；   
　　（5）限制训练时间；通过评估测试；   
　　（6）增加噪声 Noise： 输入时+权重上（高斯初始化） ；

　　（7）数据清洗(data ckeaning/Pruning)：将错误的label 纠正或者删除错误的数据。

　　（8）结合多种模型： Bagging用不同的模型拟合不同部分的训练集；Boosting只使用简单的神经网络；

　　**产生过拟合根本原因：**

　　**1、 观察值与真实值存在偏差：**   
　　 训练样本的获取，本身就是一种 抽样。抽样操作就会存在误差， 也就是你的训练样本 取值 X， X = x(真值) + u（随机误差)，机器学习的 优化函数 多为 min Cost函数，自然就是尽可能的拟合 X，而不是真实的x,所以 就称为过拟合了，实际上是学习到了真实规律以外的 随机误差。举个例子说，你想做人脸识别，人脸里有背景吧，要是你这批人脸背景A都相似，学出来的模型，见到背景A，就会认为是人脸。这个背景A就是你样本引入的误差。  
　　**2、 数据太少，导致无法描述问题的真实分布**  
 　　举个例子，投硬币问题 是一个 二项分布，但是如果 你碰巧投了10次，都是正面。那么你根据这个数据学习，是无法揭示这个规律的，根据统计学的大数定律（通俗地说，这个定理就是，在试验不变的条件下，重复试验多次，随机事件的频率近似于它的概率），当样本多了，这个真实规律是必然出现的。  
　　为什么说 数据量大了以后 就能防止过拟合，数据量大了，  
    问题2，不再存在，  
    问题1，在求解的时候因为数据量大了， 求解min Cost函数时候， 模型为了求解到最小值过程中，需要兼顾真实数据拟合 和 随机误差拟合，所有样本的真实分布是相同的（都是人脸），而随机误差会一定程度上抵消（背景），

　　**（1）数据有噪声。**

　　我们可以理解地简单些：有噪音时，**更复杂的模型会尽量去覆盖噪音点，即对数据过拟合**。这样，即使训练误差Ein 很小（接近于零），由于没有描绘真实的数据趋势，Eout 反而会更大。  
　　即噪音严重误导了我们的假设。还有一种情况，**如果数据是由我们不知道的某个非常非常复杂的模型产生的，实际上有限的数据很难去“代表”这个复杂模型曲线**。我们采用不恰当的假设去尽量拟合这些数据，效果一样会很差，因为部分数据对于我们不恰当的复杂假设就像是“噪音”，误导我们进行过拟合。

　　如下面的例子，假设数据是由50次幂的曲线产生的（下图右边），与其通过10次幂的假设曲线去拟合它们，还不如采用简单的2次幂曲线来描绘它的趋势。

![](https://images2018.cnblogs.com/blog/1393464/201807/1393464-20180728160447403-2089982258.jpg)

　　**（2）训练数据不足，有限的训练数据。**

　　**（3）训练模型过度，导致模型非常复杂。**

**2、正则方法主要有哪些？**

（1）**L1和L2正则：**都是针对模型中参数过大的问题引入惩罚项，依据是奥克姆剃刀原理。在深度学习中，L1会趋向于产生少量的特征，而其他的特征都是0增加网络稀疏性；而L2会选择更多的特征，这些特征都会接近于0，防止过拟合。神经网络需要每一层的神经元尽可能的提取出有意义的特征，而这些特征不能是无源之水，因此L2正则用的多一些。

（2）**dropout：**深度学习中最常用的正则化技术是dropout，随机的丢掉一些神经元。

（3）**数据增强，**比如将原始图像翻转平移拉伸，从而是模型的训练数据集增大。数据增强已经是深度学习的必需步骤了，其对于模型的泛化能力增加普遍有效，但是不必做的太过，将原始数据量通过数据增加增加到2倍可以，但增加十倍百倍就只是增加了训练所需的时间，不会继续增加模型的泛化能力了。

（4）**提前停止（early stopping）：**就是让模型在训练的差不多的时候就停下来，比如继续训练带来提升不大或者连续几轮训练都不带来提升的时候，这样可以避免只是改进了训练集的指标但降低了测试集的指标。

（5）**批量正则化（BN）：**就是将卷积神经网络的每层之间加上将神经元的权重调成标准正态分布的正则化层，这样可以让每一层的训练都从相似的起点出发，而对权重进行拉伸，等价于对特征进行拉伸，在输入层等价于数据增强。注意正则化层是不需要训练。