#模型不收敛的原因

1.数据量少的话，一般不会带来不收敛
不收敛一般是由于样本的信息量太大导致网络不足以fit住整个样本空间，样本少只可能带来过拟合的问题。
降低过拟合的方法：比如dropout，SGD，增大minibatch的数量，减少fc层的节点数量，momentum，finetune等。

2.尽量收集更多的数据

3.尽可能用小模型
数据量太少应考虑缩小模型复杂度。考虑减少层数或者减少kernel number



#tensorflow使用高阶api导致训练不收敛问题




>https://blog.csdn.net/chongtong/article/details/90369917?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-21&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-21


上图是tensorflow软件栈图，我之前学习和实现的网络模型（0.12a）使用的是 低级api, 现在的新版本（1.10）对低级api进行了封装，形成了高级api(estimator keras)，所以对原有模型进行了一次api替换

### 移植

整个代码的移植过程还是比较顺利的，移植完成以后，代码量比以前减少了很多，但移植完成，在运行时却发现了一些奇怪的现象

### 问题现象

我这里有两个自己的数据集，其中一个数据集（单个数据量130）在移植后的代码上运行正常，另一个数据集（单个数据集60000）在移植后的代码上运行却出现以下问题：

-   二分类问题的准确率在50%左右
-   训练过程中loss值会变化到一个固定值，然后就不再变化了


### 问题分析

因为不同数据集对应的结论不同，所以问题的排查就主要集中在对比两份代码的差异上了。训练正常的代码简称为T代码，训练异常的代码简称为F代码

-   网络排查
    
    > 怀疑模型代码有问题。将F代码尽量用T代码代替，最终替换后，只剩下tfrecord文件读取和解析、网络输入层不一样，然后再训练更新后的F代码，现象依然存在
    
-   数据排查
    
    > 通过网络排查基本排除网络问题，唯一不同在于数据，于是进行数据验证。将F代码训练过程中的输入数据记录到文件，然后对比F代码读到的数据和制作tfrecord的数据。排查数据编号、数据内容是否一致。最终发现数据一致。


新代码中使用的损失函数是`tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)`，搜索并替换为[tensorflow官方module库](https://github.com/tensorflow/models)中的损失函数`tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)`再进行训练，发现一切正常





#SSD-TensorFlow不收敛的问题
第一次发~  
主要是解决初学者使用SSD-TensorFlow不收敛的问题。  
[SSD-TensorFlow下载地址](https://github.com/balancap/SSD-Tensorflow)  
[加载模型对图片进行测试](https://blog.csdn.net/duanyajun987/article/details/82786953)

许多初学者在训练时会发现SSD-TensorFlow的loss非常大的问题，其实github里已有人找到这个问题并解决了。具体做法是将nets->ssd\_vgg\_300.py的637-647行，如下代码 # Add cross-entropy loss.  
with tf.name\_scope(‘cross\_entropy\_pos’):  
loss = tf.nn.sparse\_softmax\_cross\_entropy\_with\_logits(logits=logits,  
labels=gclasses)  
loss = tf.div(tf.reduce\_sum(loss \* fpmask), batch\_size, name=‘value’)  
tf.losses.add\_loss(loss)  
with tf.name\_scope(‘cross\_entropy\_neg’):  
loss = tf.nn.sparse\_softmax\_cross\_entropy\_with\_logits(logits=logits,  
labels=no\_classes)  
loss = tf.div(tf.reduce\_sum(loss \* fnmask), batch\_size, name=‘value’)  
tf.losses.add\_loss(loss)  
改为：  
fn\_neg = tf.cast(n\_neg, dtype)  
fn\_positives = tf.cast(n\_positives, dtype)  
with tf.name\_scope(‘cross\_entropy\_pos’):  
loss = tf.nn.sparse\_softmax\_cross\_entropy\_with\_logits(logits=logits,  
labels=gclasses)  
loss = tf.div(tf.reduce\_sum(loss \* fpmask), fn\_positives, name=‘value’)  
tf.losses.add\_loss(loss)  
with tf.name\_scope(‘cross\_entropy\_neg’):  
loss = tf.nn.sparse\_softmax\_cross\_entropy\_with\_logits(logits=logits,  
labels=no\_classes)  
loss = tf.div(tf.reduce\_sum(loss \* fnmask), fn\_neg, name=‘value’)  
tf.losses.add\_loss(loss)  
具体解释在github里，这里附上链接[模型收敛问题](https://github.com/balancap/SSD-Tensorflow/issues/279)



#深度学习训练时网络不收敛的原因分析总结

很多同学会发现，为什么我训练网络的时候loss一直居高不下或者准确度时高时低，震荡趋势，一会到11，一会又0.1，不收敛。 又不知如何解决，博主总结了自己训练经验和看到的一些方法。

首先你要保证训练的次数够多，不要以为一百两百次就会一直loss下降或者准确率一直提高，会有一点震荡的。只要总体收敛就行。若训练次数够多（一般上千次，上万次，或者几十个epoch）没收敛，则试试下面方法：

## **1\. 数据和标签**

数据分类标注是否准确？数据是否干净？数据库太小一般不会带来不收敛的问题，只要你一直在train总会收敛（rp问题跑飞了不算）。反而不收敛一般是由于样本的信息量太大导致网络不足以fit住整个样本空间。样本少只可能带来过拟合的问题

## 2\. 学习率设定不合理

在自己训练新网络时，可以从0.1开始尝试，如果loss不下降的意思，那就降低，除以10，用0.01尝试，一般来说0.01会收敛，不行的话就用0.001. 学习率设置过大，很容易震荡。不过刚刚开始不建议把学习率设置过小，尤其是在训练的开始阶段。在开始阶段我们不能把学习率设置的太低否则loss不会收敛。我的做法是逐渐尝试，从0.1,0.08,0.06,0.05 ......逐渐减小直到正常为止，

有的时候候学习率太低走不出低估，把冲量提高也是一种方法，适当提高mini-batch值，使其波动不大。,

##  **3.网络设定不合理**

如果做很复杂的分类任务，却只用了很浅的网络，可能会导致训练难以收敛，换网络换网络换网络，重要的事情说三遍，或者也可以尝试加深当前网络。

## 4.数据集label的设置

检查lable是否有错，有的时候图像类别的label设置成1，2，3正确设置应该为0,1,2。

## 5、改变图片大小

博主看到一篇文章，说改变图片大小可以解决收敛问题，具体博主没试过，只看到有这个方法，具体文章链接：[https://blog.csdn.net/Fighting\_Dreamer/article/details/71498256](https://blog.csdn.net/Fighting_Dreamer/article/details/71498256)

感兴趣的可以去看看。

## 6、数据归一化

神经网络中对数据进行归一化是不可忽略的步骤，网络能不能正常工作，还得看你有没有做归一化，一般来讲，归一化就是减去数据平均值除以标准差，通常是针对每个输入和输出特征进行归一化




#有可能的原因之一
不请自来，随便逛看到了这个问题没人回答。

导致不收敛情况的原因有很多种，其中一种是一些样本正向传播到最外层时输出为0了，导致log(0)的出现，随之而来的就是loss为nan，这也是一个小陷阱。

根据题主给的代码，我认为这个原因导致问题出现的可能性比较大。

解决办法也比较有意思，就是给log里加一个小余项，具体到题主的代码就是：

将cross\_entropy = -tf.reduce\_sum(y\_\*tf.log(y))  
改为：  
cross\_entropy = -tf.reduce\_sum(y\_\*tf.log(y+1e-10))

如果一切顺利，应该不会出现loss为nan的情况了，题主可以试一试~



#内存泄漏

计算机配置内存12G，显存4G，运行有10分钟左右就提醒说内存不够用然后退出运行，直觉是程序bug问题
终于找到靠谱解决方案：
http://cherishlc.iteye.com/blog/2324796
这个博客的第二个给了大致的讲解，在给出的附录中：
https://stackoverflow.com/documentation/tensorflow/3883/how-to-debug-a-memory-leak-in-tensorflow/13426/use-graph-finalize-to-catch-nodes-being-added-to-the-graph#t=201707221208374841351
讲解更详细，并给出了解决的方法
即：sess.graph.finalize() 使得整个graph变为只读的，不能再向图中添加任何节点
我也明白了我的错误原因，在for循环中用了tf.convert_to_tensor方法，相当于不停地向图中添加节点，谢谢给出了解决方案的大牛们
另一个附录：https://github.com/tensorflow/tensorflow/issues/4151
通过一个示例给出了可行的替换方法，非常具有可行性




session和graph没有释放内存。使用了with关键字可以在session异常退出时也释放内存，否则要用session.close()关闭session。代码如下：

session一定要释放：

```
with tf.Seesion() as session:
    # 在session内部加载保存好的graph
    saver = tf.train.import_meta_graph('./crack_captcha.model-9900.meta')
    saver.restore(session, "./crack_captcha.model-9900.meta")
    # codes
```

在session中加载graph（训练好的模型），导致每次关闭程序再运行，graph出现重复加载的现象（尤其是重复加载模型的时候容易导致内存溢出）：

```
# 用with新建一个graph，这样在运行完以及异常退出时就会释放内存
graph = tf.Graph()
with graph.as_default():
    saver = tf.train.import_meta_graph('./crack_captcha.model-9900.meta')
    with tf.Session(graph=graph) as session:
        saver.restore(session, "./CNN_cracks")
```